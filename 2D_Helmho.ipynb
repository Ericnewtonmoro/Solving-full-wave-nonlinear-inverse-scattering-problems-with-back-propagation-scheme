{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiMbyI38Tuml96XN9RAppe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ericnewtonmoro/Solving-full-wave-nonlinear-inverse-scattering-problems-with-back-propagation-scheme/blob/master/2D_Helmho.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBOdE18v-tRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22d49c7-2436-4321-b762-e05dadc7cbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pinnsformer' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/AdityaLab/pinnsformer\n",
        "\n",
        "\n",
        "\n",
        "# Import sys and the repository to the path\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_path = \"/content/pinnsformer\"\n",
        "sys.path.append(repo_path)"
      ],
      "metadata": {
        "id": "XB7vgfED-zLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.optim import LBFGS, Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "from util import *\n",
        "from model.pinn import PINNs\n",
        "from model.pinnsformer import PINNsformer"
      ],
      "metadata": {
        "id": "zNqyg-oM_LY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "step_size = 1e-4"
      ],
      "metadata": {
        "id": "yccO2Uw4_fh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QC7Q0jA_u9S",
        "outputId": "85cd2eee-d19d-4b63-feb8-9297674b7a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 14 06:16:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0              30W /  70W |    127MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data for the 2D Helmholtz problem\n",
        "def get_2d_data(x_range, y_range, nx, ny):\n",
        "    x = np.linspace(x_range[0], x_range[1], nx)\n",
        "    y = np.linspace(y_range[0], y_range[1], ny)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    res = np.stack([xx.flatten(), yy.flatten()], axis=1)\n",
        "    b_left = res[xx.flatten() == x_range[0]]\n",
        "    b_right = res[xx.flatten() == x_range[1]]\n",
        "    b_upper = res[yy.flatten() == y_range[1]]\n",
        "    b_lower = res[yy.flatten() == y_range[0]]\n",
        "    return res, b_left, b_right, b_upper, b_lower"
      ],
      "metadata": {
        "id": "SjVyQuJnBRJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get training and test data\n",
        "res, b_left, b_right, b_upper, b_lower = get_2d_data([0,1], [0,1], 51, 51)\n",
        "res_test, _, _, _, _ = get_2d_data([0,1], [0,1], 101, 101)"
      ],
      "metadata": {
        "id": "dM_46dCTDvWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare time sequences\n",
        "#def make_time_sequence(data, num_step=5, step=step_size):\n",
        "    #time_seq = []\n",
        "    #for i in range(num_step):\n",
        "        #t = i * step * np.ones((data.shape[0], 1))\n",
        "        #time_seq.append(np.hstack([data, t]))\n",
        "    #return np.stack(time_seq, axis=1)\n",
        "\n",
        "def make_time_sequence(data, num_step=5, step=step_size):\n",
        "    time_seq = []\n",
        "    # Convert data to numpy array if it's a tensor\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.detach().cpu().numpy()  # Move tensor to CPU and convert to NumPy array\n",
        "    for i in range(num_step):\n",
        "        t = i * step * np.ones((data.shape[0], 1))\n",
        "        #t = t[:, np.newaxis, np.newaxis]\n",
        "        if len(data.shape) == 2:\n",
        "           time_seq.append(np.concatenate([data,t], axis=1))\n",
        "        else:\n",
        "           t = t.reshape(data.shape[0], 1, 1)\n",
        "           t = t.repeat(data.shape[1], axis=1)\n",
        "           time_seq.append(np.concatenate([data, t], axis=2))\n",
        "    return np.stack(time_seq, axis=1)\n",
        "\n",
        "res = make_time_sequence(res, num_step=5, step=step_size)\n",
        "b_left = make_time_sequence(b_left, num_step=5, step=step_size)\n",
        "b_right = make_time_sequence(b_right, num_step=5, step=step_size)\n",
        "b_upper = make_time_sequence(b_upper, num_step=5, step=step_size)\n",
        "b_lower = make_time_sequence(b_lower, num_step=5, step=step_size)\n",
        "\n",
        "# Convert to tensors\n",
        "res = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(device)\n",
        "b_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(device)\n",
        "b_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(device)\n",
        "b_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(device)\n",
        "b_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(device)\n",
        "\n",
        "x_res, y_res, t_res = res[:,:,0:1], res[:,:,1:2], res[:,:,2:3]\n",
        "x_left, y_left, t_left = b_left[:,:,0:1], b_left[:,:,1:2], b_left[:,:,2:3]\n",
        "x_right, y_right, t_right = b_right[:,:,0:1], b_right[:,:,1:2], b_right[:,:,2:3]\n",
        "x_upper, y_upper, t_upper = b_upper[:,:,0:1], b_upper[:,:,1:2], b_upper[:,:,2:3]\n",
        "x_lower, y_lower, t_lower = b_lower[:,:,0:1], b_lower[:,:,1:2], b_lower[:,:,2:3]\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "model = PINNsformer(d_out=1, d_hidden=512, d_model=32, N=1, heads=2).to(device)\n",
        "model.apply(init_weights)\n",
        "optim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n",
        "\n",
        "print(model)\n",
        "print(get_n_params(model))\n",
        "\n",
        "loss_track = []\n",
        "\n",
        "pi = torch.tensor(np.pi, dtype=torch.float32, requires_grad=False).to(device)\n",
        "\n",
        "for i in tqdm(range(1000)):\n",
        "    def closure():\n",
        "        input_res = torch.cat([x_res, y_res, t_res], dim=-1) # Concatenate x, y, and t into a single tensor\n",
        "        input_left = torch.cat([x_left, y_left, t_left], dim=-1)\n",
        "        input_right = torch.cat([x_right, y_right, t_right], dim=-1)\n",
        "        input_upper = torch.cat([x_upper, y_upper, t_upper], dim=-1)\n",
        "        input_lower = torch.cat([x_lower, y_lower, t_lower], dim=-1)\n",
        "\n",
        "        pred_res = model(input_res) # Pass the concatenated tensor to the model\n",
        "        pred_left = model(input_left)\n",
        "        pred_right = model(input_right)\n",
        "        pred_upper = model(input_upper)\n",
        "        pred_lower = model(input_lower)\n",
        "        #pred_res = model(x_res, y_res, t_res)\n",
        "        #pred_left = model(x_left, y_left, t_left)\n",
        "        #pred_right = model(x_right, y_right, t_right)\n",
        "        #pred_upper = model(x_upper, y_upper, t_upper)\n",
        "        #pred_lower = model(x_lower, y_lower, t_lower)\n",
        "\n",
        "        u_x = torch.autograd.grad(pred_res, x_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
        "        u_xx = torch.autograd.grad(u_x, x_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
        "        u_y = torch.autograd.grad(pred_res, y_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
        "        u_yy = torch.autograd.grad(u_y, y_res, grad_outputs=torch.ones_like(pred_res), retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "        # Helmholtz equation: ∇²ψ + k²ψ = f(x, y)\n",
        "        k = 2 * pi\n",
        "        f_xy = torch.sin(pi * x_res) * torch.sin(pi * y_res)\n",
        "        loss_res = torch.mean((u_xx + u_yy + k**2 * pred_res - f_xy) ** 2)\n",
        "\n",
        "        loss_bc = torch.mean((pred_upper) ** 2) + torch.mean((pred_lower) ** 2) + torch.mean((pred_left) ** 2) + torch.mean((pred_right) ** 2)\n",
        "\n",
        "        loss_track.append([loss_res.item(), loss_bc.item()])\n",
        "\n",
        "        loss = loss_res + loss_bc\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optim.step(closure)\n",
        "\n",
        "print('Loss Res: {:4f}, Loss_BC: {:4f}'.format(loss_track[-1][0], loss_track[-1][1]))\n",
        "print('Train Loss: {:4f}'.format(np.sum(loss_track[-1])))\n",
        "\n",
        "torch.save(model.state_dict(), './2dhelmholtz_pinnsformer.pt')"
      ],
      "metadata": {
        "id": "Zv1vRZDAD0bq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5a501e3-ee11-43c6-f44d-02a5f2941faf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PINNsformer(\n",
            "  (linear_emb): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (encoder): Encoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): EncoderLayer(\n",
            "        (attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (linear): Sequential(\n",
            "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
            "            (1): WaveAct()\n",
            "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (3): WaveAct()\n",
            "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (act1): WaveAct()\n",
            "        (act2): WaveAct()\n",
            "      )\n",
            "    )\n",
            "    (act): WaveAct()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (ff): FeedForward(\n",
            "          (linear): Sequential(\n",
            "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
            "            (1): WaveAct()\n",
            "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (3): WaveAct()\n",
            "            (4): Linear(in_features=256, out_features=32, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (act1): WaveAct()\n",
            "        (act2): WaveAct()\n",
            "      )\n",
            "    )\n",
            "    (act): WaveAct()\n",
            "  )\n",
            "  (linear_out): Sequential(\n",
            "    (0): Linear(in_features=32, out_features=512, bias=True)\n",
            "    (1): WaveAct()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): WaveAct()\n",
            "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "453561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PINNsformer.forward() missing 1 required positional argument: 't'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-089ac67708c0>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss Res: {:4f}, Loss_BC: {:4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_track\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_track\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-089ac67708c0>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minput_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lower\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mpred_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass the concatenated tensor to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mpred_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpred_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PINNsformer.forward() missing 1 required positional argument: 't'"
          ]
        }
      ]
    }
  ]
}